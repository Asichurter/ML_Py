{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "a = Variable(t.randn(3,4))\n",
    "b = Variable(t.randn(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7352, -0.5643,  0.6442,  0.4008],\n",
      "        [ 0.4777, -1.3136,  0.4552, -0.9142],\n",
      "        [ 0.4516,  2.1523, -0.8314, -1.9273]])\n",
      "tensor([[-0.9312, -0.4201,  0.7684, -2.1130],\n",
      "        [ 0.3719,  1.1706,  0.0960,  0.7285],\n",
      "        [-1.8437,  1.5055,  0.0727, -0.9754]])\n",
      "tensor([[-0.7352, -0.5643,  0.6442,  0.4008],\n",
      "        [ 0.4777, -1.3136,  0.4552, -0.9142],\n",
      "        [ 0.4516,  2.1523, -0.8314, -1.9273]])\r",
      "tensor([[-0.9312, -0.4201,  0.7684, -2.1130],\n",
      "        [ 0.3719,  1.1706,  0.0960,  0.7285],\n",
      "        [-1.8437,  1.5055,  0.0727, -0.9754]])\n"
     ]
    }
   ],
   "source": [
    "print(a,b,sep='\\n')\n",
    "a.add(b)\n",
    "print(a,b,sep='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.张量的基本运算\n",
    "pytorch中使用三种方式进行基本运算：\n",
    "1. 直接使用符号\n",
    "```python\n",
    "c = a + b\n",
    "```\n",
    "2. 使用torch.add\n",
    "```python\n",
    "   c = torch.add(a,b)\n",
    "```\n",
    "3. 使用a.add\n",
    "```python\n",
    "   c = a.add(b)\n",
    "```\n",
    "注意，这种方式相加的时候，a不会随之改变，如果需要随之改变，使用\n",
    "```python\n",
    "   c = a.add_(b)\n",
    "   torch.add(a, b, result=c)\n",
    "```\n",
    "\n",
    "torch支持许多类似的操作，如果不支持的话，可以先转化为numpy进行操作，再转化\n",
    "回tensor\n",
    "\n",
    "方法是：\n",
    "使用tensor.numpy()\n",
    "\n",
    "从numpy转为tensor时使用torch.from_numpy(a)返回一个tensor。这两者是共用内存的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "grad_test = Variable(t.ones(2,2), requires_grad=True)\n",
    "total = 2*grad_test.sum()\n",
    "total.backward(retain_graph=True)\n",
    "print(grad_test.grad)\n",
    "print(grad_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.自动求导\n",
    "每一个Variable类内部有一个包含数据的tensor:data，还有一个用于保存梯度的tensor：grad。\n",
    "\n",
    "在定义了张量之间的运算后，可以通过调用**结果的backward()方法**来使与其有关的张量上的grad\n",
    "\n",
    "梯度被计算出来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.]]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[1., 1.]],\n",
      "\n",
      "        [[1., 1.]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "unsqueeze,unsqueeze_ = grad_test.unsqueeze(2),grad_test.unsqueeze(1)\n",
    "print(unsqueeze)\n",
    "print(unsqueeze_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用unsqueeze来增加一个维度\n",
    "unsqueeze将会在指定张量的指定的轴上增加一个维度，例如：\n",
    "```python\n",
    "    a = a.unsqueeze(1)\n",
    "```\n",
    "将会在a向量的第二维（序号为一）的维度上增加一个维度，即用一个维度包裹原来的该维度\n",
    "\n",
    "同理，可以使用squeeze来压缩一个维度\n",
    "\n",
    "如果不在squeeze中给定参数，将会把所有为1维的维度压缩掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[1., 3.],\n",
      "        [4., 6.],\n",
      "        [7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "#源数据\n",
    "inputs = t.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "#指定选择的下标\n",
    "indexes = t.LongTensor([0,2])\n",
    "#指定不同的维度上进行选择\n",
    "print(inputs.index_select(0, indexes))\n",
    "print(inputs.index_select(1, indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.masked_select(inputs>5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.masked_select\n",
    "利用二进制掩码序列，找到1对应的位置的值作为一维张量返回\n",
    "\n",
    "上例中，inputs>5 产生了一个与原张量形状相同，但是只有对应位置上的值大于5才会为1否则为0的张量\n",
    "\n",
    "利用这个掩码张量，返回的所有位置为1的值便是在返回所有大于5的值，以一维张量的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 5., 9.]])\n",
      "tensor([[-1.,  2.,  3.],\n",
      "        [ 4., -1.,  6.],\n",
      "        [ 7.,  8., -1.]])\n"
     ]
    }
   ],
   "source": [
    "gather_diag = t.LongTensor([[0,1,2]])\n",
    "print(inputs.gather(0, gather_diag))\n",
    "print(inputs.scatter(0, gather_diag, t.Tensor([[-1,-1,-1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.gather与scatter\n",
    "gather聚集函数利用一个index向量在指定的轴上，来获取源输入中的部分向量组成新张量，输入张量的大小与index一致\n",
    "\n",
    "获取的方式是：\n",
    "```python\n",
    "    out[i][j] = input[index[i][j]][j] #dim=0\n",
    "    out[i][j] = input[i][index[i][j]] #dim=1\n",
    "```\n",
    "即：指定的dim是index作用的域，其他轴都遍历。利用的坐标与输出的坐标位置相同\n",
    "\n",
    "scatter作用与gather刚好相反，将指定的值按轴以index指定的形式放回指定张量中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([1., 9.])\n",
      "tensor([2., 8.])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "#高级索引\n",
    "#高级索引不会共享内存\n",
    "print(inputs)\n",
    "print(inputs[[0,2],[0,2]])\n",
    "print(inputs[[0,2],[1]])\n",
    "print(inputs[...,[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.],\n",
      "        [15.],\n",
      "        [24.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sum(dim=1, keepdim=True))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.使用归并操作\n",
    "sum，mean，norm，std等操作能够将输入的一些值进行约减，关键在于要指定轴\n",
    "\n",
    "如果一个i,j,k的矩阵指定dim=0，则输出为1,j,k。即指定的dim就是最终为1的轴，其对应的另外一些轴上的值将被约减。\n",
    "\n",
    "例如指定dim=1，则相当于dim=0上的值被约减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 1, 1]], dtype=torch.uint8)\n",
      "tensor([6., 7., 8., 9.])\n",
      "tensor(9.)\n",
      "(tensor([7., 8., 9.]), tensor([2, 2, 2]))\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "compare = t.ones((3,3)) + 4\n",
    "#直接对同尺寸大小的张量进行比较操作，将会得到一个逐元素比较得到的byte矩阵\n",
    "print(inputs>compare)\n",
    "#使用这个byte矩阵可以从原矩阵中获得对应位置的数据\n",
    "print(inputs[inputs>compare])\n",
    "print(inputs.max())\n",
    "#max的dim指定的是比较的维度，这个维度就是显式的维度\n",
    "#返回值是一个元组，第一个元素是最大值，第二个是下标\n",
    "print(inputs.max(dim=0))\n",
    "#获得两个张量中，每个位置最大元素组成的新张量\n",
    "print(inputs.max(compare))\n",
    "#还可以使用clamp来指定一个固定的最大/最小值\n",
    "print(inputs.clamp(min=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4., 7.],\n",
      "        [2., 5., 8.],\n",
      "        [3., 6., 9.]])\n",
      "False\n",
      "tensor([[1., 4., 7.],\n",
      "        [2., 5., 8.],\n",
      "        [3., 6., 9.]])\n",
      "tensor([[1., 4., 7.],\n",
      "        [2., 5., 8.],\n",
      "        [3., 6., 9.]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.t())\n",
    "print(inputs.t().is_contiguous())\n",
    "print(inputs.t().contiguous())\n",
    "print(inputs.transpose(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.矩阵转置\n",
    "torch.t()和torch.transpose()都可以实现矩阵的转置，不同的是，前者只能转置2D矩阵，而后者可以转置任意维度\n",
    "\n",
    "的矩阵\n",
    "\n",
    "transpose需要两个参数，即两个需要交换的维度的下标\n",
    "\n",
    "t方法制造的矩阵不是连续的，需要使用contiguous方法变为连续的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 1., 2., 3.],\n",
      "        [4., 5., 6., 4., 5., 6.],\n",
      "        [7., 8., 9., 7., 8., 9.],\n",
      "        [1., 2., 3., 1., 2., 3.],\n",
      "        [4., 5., 6., 4., 5., 6.],\n",
      "        [7., 8., 9., 7., 8., 9.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "test = t.Tensor([[1],[2],[3]])\n",
    "print(inputs.repeat(2,2))   #将各个维度上扩大多少倍\n",
    "print(test.expand(3,3))     #将张量扩张为何种大小的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.repeat和expand\n",
    "两者都是可以将张量内部的值进行重复化，不同的是，expand只能对一个含有单一值的维度进行扩张\n",
    "\n",
    "但是repeat没有这个限制。前者更加节省内存而已。\n",
    "\n",
    "参数：\n",
    "repeat：各个维度上重复多少次\n",
    "\n",
    "expand：指定各个维度的大小，可以使用-1来自适应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.3572, 30.0671],\n",
      "        [31.4863, 30.5066]])\n",
      "tensor([[ 7.3572, 30.0671],\n",
      "        [31.4863, 30.5066]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_a = t.ones((2,2), requires_grad=True)\n",
    "grad_b = t.randn((2,2))\n",
    "grad_c = 2*grad_a**2 + grad_b\n",
    "grad_d = (grad_c**2).sum()\n",
    "grad_d.backward()\n",
    "print(grad_a.grad)\n",
    "#手动求导的结果\n",
    "print(2*grad_c*4*grad_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.使用自动求导机制\n",
    "1. 直接创建tensor而不是Variable，因为后者已经被废除。在需要计算向量的tensor的定义处加上requires_grad=True\n",
    "2. 尽管有些变量没有声音需要梯度，但是由于有些需要梯度的张量的计算依赖于它，所以还是会计算张量，但是之后就被丢弃\n",
    "3. 需要从计算末尾进行链式法则的反向求导。注意：结果必须是一个标量scalar。其实也可以从中间的向量开始反向传播，但是需要指定目标函数对其的梯度向量，可以理解为梯度反向传播时，中间节点在已知本节点的梯度情况下向后传播梯度。方法是：指定grad_variables。由于目标函数一般是一个标量，因此grad_variables的形状与中间节点的形状一致\n",
    "4. 如果一个变量不需要求导，且路径上依赖这个变量的变量（子变量）都不需要求导，则可以设置volatile=True。而且volatile的优先级高于requires_grad。\n",
    "\n",
    "\n",
    "示例中，手动求导结果和backward()结果相同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x00000211D938FEF0>\n",
      "((<MulBackward0 object at 0x00000211D938FBA8>, 0), (None, 0))\n"
     ]
    }
   ],
   "source": [
    "#a为叶节点，没有操作函数\n",
    "print(grad_a.grad_fn)\n",
    "#c是一个中间节点，包含多个操作函数，因为是多项式\n",
    "print(grad_c.grad_fn)\n",
    "#展示c中的所有操作函数\n",
    "print(grad_c.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10.自动求导细则\n",
    "在底层的实现中，每一个tensor都包含一个实例grad和一个操作实例grad_fn，前者用于保存某一次backward()时计算得到的梯度，后者用于记录对应的操作来计算特定的梯度。在每一次backward()进行之后，需要梯度的节点会保留梯度在grad中，但是不需要梯度的中间变量即使会在backwar()过程中计算梯度，但是会在计算结束后删除掉。如果要获取中间变量的梯度，有两种方式：\n",
    "1. 使用hook:自定义一个hook的函数，输入是梯度，没有输出值。将这个hook利用register_hook()注册到对应的变量上，使用完以后移除\n",
    "```python\n",
    "    def hook(grad):\n",
    "        print(grad)\n",
    "    handle_hook = y.register_hook(hook)\n",
    "    #...\n",
    "    handle_hook.remove()\n",
    "```\n",
    "2. 使用autograd.grad方法，重新计算grad:\n",
    "```python\n",
    "    grad = t.autograd.grad(z, y) #z对y求偏导\n",
    "```\n",
    "\n",
    "如果遭遇不能自动求导的操作，则需要自己写出自定义的autograd.Function来对应。使用的时候，使用类的apply方法\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0096, 0.1442],\n",
      "        [0.1324, 0.2317]])\n",
      "tensor([[0.0096, 0.1442],\n",
      "        [0.1324, 0.2317]])\n"
     ]
    }
   ],
   "source": [
    "class MulSqureAdd(t.autograd.Function):\n",
    "    #两个方法：forward和backward的输入互相是对方的输出，输出是互相的输入\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b, x_grad=True):\n",
    "        #参数:ctx是一个固定参数，w是系数，b是偏置\n",
    "        #保存是否需要对x求导的指示变量\n",
    "        ctx.x_grad = x_grad\n",
    "        #将到时候backward()时需要的变量w和x储存起来\n",
    "        ctx.save_for_backward(w,x)\n",
    "        return w*w*x+b\n",
    "    \n",
    "    @staticmethod\n",
    "    #由于forward一个输出，因此一个输入\n",
    "    #这个grad_output是目标函数值传递到下一层时的梯度，本节点的梯度需要乘该值\n",
    "    def backward(ctx, grad_output):\n",
    "        #读取保存的变量\n",
    "        x_grad = ctx.x_grad\n",
    "        #从saved_tensors中读取利用save_for_backward()储存的变量\n",
    "        w,x = ctx.saved_tensors\n",
    "        grad_w = grad_output*x*w*2\n",
    "        if x_grad:\n",
    "            grad_x = grad_output*w*w\n",
    "        else:\n",
    "            grad_x = None\n",
    "        grad_b = grad_output * 1 #因为b的系数为1\n",
    "        return grad_w,grad_x,grad_b,None   #最后一个None是因为还有一个指示是否对x求导的变量，无法求导而为None\n",
    "    \n",
    "grad_1_a = t.ones((2,2), requires_grad=True)\n",
    "grad_1_b = t.randn((2,2))\n",
    "grad_1_c = t.rand((2,2))\n",
    "grad_1_d = MulSqureAdd.apply(grad_1_b, grad_1_a, grad_1_c).sum()    #用apply来使用自定义的操作函数\n",
    "grad_1_d.backward()\n",
    "#自动求导和手动求导结果相同\n",
    "print(grad_1_a.grad)\n",
    "print(grad_1_b*grad_1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
